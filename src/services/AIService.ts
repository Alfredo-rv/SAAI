/**
 * Servicio de IA Real - Integraci√≥n con APIs de IA
 */

import OpenAI from 'openai';

export interface AIProvider {
  name: string;
  available: boolean;
  models: string[];
  rateLimit?: {
    requestsPerMinute: number;
    tokensPerMinute: number;
  };
}

export interface AIRequest {
  provider: 'openai' | 'anthropic' | 'local';
  model: string;
  prompt: string;
  context?: any;
  temperature?: number;
  maxTokens?: number;
}

export interface AIResponse {
  content: string;
  usage?: {
    promptTokens: number;
    completionTokens: number;
    totalTokens: number;
  };
  model: string;
  provider: string;
  timestamp: Date;
}

export class AIService {
  private openai?: OpenAI;
  private providers: Map<string, AIProvider> = new Map();
  private requestHistory: Array<{ request: AIRequest; response: AIResponse }> = [];

  constructor() {
    this.initializeProviders();
  }

  private initializeProviders(): void {
    // OpenAI
    this.providers.set('openai', {
      name: 'OpenAI',
      available: false, // Se activar√° cuando se configure la API key
      models: ['gpt-4', 'gpt-3.5-turbo', 'gpt-4-turbo'],
      rateLimit: {
        requestsPerMinute: 60,
        tokensPerMinute: 150000
      }
    });

    // Anthropic
    this.providers.set('anthropic', {
      name: 'Anthropic Claude',
      available: false,
      models: ['claude-3-sonnet', 'claude-3-haiku'],
      rateLimit: {
        requestsPerMinute: 50,
        tokensPerMinute: 100000
      }
    });

    // Local (simulado)
    this.providers.set('local', {
      name: 'Local AI',
      available: true,
      models: ['local-llm'],
      rateLimit: {
        requestsPerMinute: 1000,
        tokensPerMinute: 1000000
      }
    });
  }

  async configureProvider(provider: string, apiKey: string): Promise<boolean> {
    try {
      switch (provider) {
        case 'openai':
          this.openai = new OpenAI({ 
            apiKey,
            dangerouslyAllowBrowser: true // Solo para demo - en producci√≥n usar backend
          });
          
          // Verificar conexi√≥n
          await this.openai.models.list();
          
          const openaiProvider = this.providers.get('openai')!;
          openaiProvider.available = true;
          
          console.log('‚úÖ OpenAI configurado correctamente');
          return true;

        case 'anthropic':
          // Configurar Anthropic cuando est√© disponible
          console.log('üöß Anthropic en desarrollo');
          return false;

        default:
          return false;
      }
    } catch (error) {
      console.error(`‚ùå Error configurando ${provider}:`, error);
      return false;
    }
  }

  async generateResponse(request: AIRequest): Promise<AIResponse> {
    console.log(`ü§ñ Generando respuesta con ${request.provider}/${request.model}`);

    switch (request.provider) {
      case 'openai':
        return await this.generateOpenAIResponse(request);
      case 'local':
        return await this.generateLocalResponse(request);
      default:
        throw new Error(`Proveedor no soportado: ${request.provider}`);
    }
  }

  private async generateOpenAIResponse(request: AIRequest): Promise<AIResponse> {
    if (!this.openai) {
      throw new Error('OpenAI no configurado');
    }

    try {
      const completion = await this.openai.chat.completions.create({
        model: request.model,
        messages: [
          {
            role: 'system',
            content: 'Eres un experto analista de datos que ayuda con an√°lisis de datasets tabulares. Responde en espa√±ol de manera clara y t√©cnica.'
          },
          {
            role: 'user',
            content: request.prompt
          }
        ],
        temperature: request.temperature || 0.7,
        max_tokens: request.maxTokens || 1000
      });

      const response: AIResponse = {
        content: completion.choices[0]?.message?.content || '',
        usage: completion.usage ? {
          promptTokens: completion.usage.prompt_tokens,
          completionTokens: completion.usage.completion_tokens,
          totalTokens: completion.usage.total_tokens
        } : undefined,
        model: request.model,
        provider: 'openai',
        timestamp: new Date()
      };

      // Guardar en historial
      this.requestHistory.push({ request, response });
      
      return response;

    } catch (error) {
      throw new Error(`Error OpenAI: ${error instanceof Error ? error.message : 'Error desconocido'}`);
    }
  }

  private async generateLocalResponse(request: AIRequest): Promise<AIResponse> {
    // Simulador local para desarrollo/demo
    await new Promise(resolve => setTimeout(resolve, 1000 + Math.random() * 2000));

    let content = '';

    // Generar respuestas contextuales basadas en el prompt
    if (request.prompt.toLowerCase().includes('analizar') || request.prompt.toLowerCase().includes('dataset')) {
      content = `He analizado el dataset y encontr√© los siguientes patrones importantes:

üîç **An√°lisis Principal:**
- El dataset contiene informaci√≥n valiosa para an√°lisis predictivo
- Se detectaron correlaciones interesantes entre variables
- La calidad de los datos es ${Math.random() > 0.5 ? 'buena' : 'excelente'}

üìä **Recomendaciones:**
- Aplicar normalizaci√≥n a variables num√©ricas
- Considerar ingenier√≠a de caracter√≠sticas adicional
- Evaluar modelos de regresi√≥n y clasificaci√≥n

üí° **Pr√≥ximos Pasos:**
- Dividir datos en entrenamiento/validaci√≥n
- Probar m√∫ltiples algoritmos
- Optimizar hiperpar√°metros`;

    } else if (request.prompt.toLowerCase().includes('modelo') || request.prompt.toLowerCase().includes('predicci√≥n')) {
      content = `Para este tipo de datos, recomiendo el siguiente enfoque de modelado:

üéØ **Estrategia de Modelado:**
- Algoritmos recomendados: Random Forest, XGBoost, Linear Regression
- Validaci√≥n cruzada con 5 folds
- M√©tricas de evaluaci√≥n: RMSE, MAE, R¬≤

‚öôÔ∏è **Configuraci√≥n Sugerida:**
- Train/Test split: 80/20
- Normalizaci√≥n: StandardScaler
- Feature selection: SelectKBest

üìà **Expectativas:**
- Precisi√≥n esperada: ${(75 + Math.random() * 20).toFixed(1)}%
- Tiempo de entrenamiento: 2-5 minutos`;

    } else {
      content = `Entiendo tu consulta sobre "${request.prompt}". 

Como sistema de IA local, puedo ayudarte con:
- An√°lisis exploratorio de datos
- Recomendaciones de modelos
- Interpretaci√≥n de resultados
- Optimizaci√≥n de pipelines

¬øTe gustar√≠a que profundice en alg√∫n aspecto espec√≠fico?`;
    }

    const response: AIResponse = {
      content,
      usage: {
        promptTokens: request.prompt.length / 4, // Estimaci√≥n
        completionTokens: content.length / 4,
        totalTokens: (request.prompt.length + content.length) / 4
      },
      model: 'local-llm',
      provider: 'local',
      timestamp: new Date()
    };

    this.requestHistory.push({ request, response });
    return response;
  }

  async analyzeDataset(data: any[], columns: any[], objective: string): Promise<string> {
    const prompt = `
Analiza este dataset y proporciona recomendaciones:

**Objetivo:** ${objective}

**Estructura del Dataset:**
- ${data.length} filas
- ${columns.length} columnas
- Tipos: ${columns.map(c => `${c.name} (${c.type})`).join(', ')}

**Estad√≠sticas Clave:**
${columns.filter(c => c.stats).map(c => {
  if (c.type === 'numeric' && 'mean' in c.stats) {
    return `- ${c.name}: Media=${c.stats.mean.toFixed(2)}, Std=${c.stats.std.toFixed(2)}`;
  } else if (c.type === 'categorical' && 'mode' in c.stats) {
    return `- ${c.name}: Modo="${c.stats.mode}", ${c.stats.topValues.length} categor√≠as`;
  }
  return `- ${c.name}: ${c.type}`;
}).join('\n')}

Proporciona:
1. An√°lisis de la calidad de datos
2. Recomendaciones de preprocesamiento
3. Algoritmos de ML sugeridos
4. M√©tricas de evaluaci√≥n apropiadas
5. Posibles problemas y soluciones
`;

    const response = await this.generateResponse({
      provider: 'local',
      model: 'local-llm',
      prompt,
      temperature: 0.3
    });

    return response.content;
  }

  getProviders(): AIProvider[] {
    return Array.from(this.providers.values());
  }

  getRequestHistory(): Array<{ request: AIRequest; response: AIResponse }> {
    return [...this.requestHistory];
  }

  getUsageStats() {
    const totalRequests = this.requestHistory.length;
    const totalTokens = this.requestHistory.reduce((sum, item) => 
      sum + (item.response.usage?.totalTokens || 0), 0
    );
    
    const providerStats = this.requestHistory.reduce((stats, item) => {
      const provider = item.request.provider;
      if (!stats[provider]) {
        stats[provider] = { requests: 0, tokens: 0 };
      }
      stats[provider].requests++;
      stats[provider].tokens += item.response.usage?.totalTokens || 0;
      return stats;
    }, {} as Record<string, { requests: number; tokens: number }>);

    return {
      totalRequests,
      totalTokens,
      providerStats,
      averageTokensPerRequest: totalRequests > 0 ? totalTokens / totalRequests : 0
    };
  }
}